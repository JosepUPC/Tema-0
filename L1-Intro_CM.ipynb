{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L1-Intro_CM.ipynb","provenance":[],"collapsed_sections":["Alky2801MFKM"],"toc_visible":true,"authorship_tag":"ABX9TyOZJ3TSFCv6kHUNyf1J2rjY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Dn2cLC7eF8uo"},"source":["# **ESEIAAT - Codificació Multimèdia**\n","## **Lab 1: Intro**\n","\n","2021 - Josep Ramon Morros - [GPI](https://imatge.upc.edu/web/) @ [IDEAI](https://ideai.upc.edu/en) Research group // ESEIAAT"]},{"cell_type":"markdown","metadata":{"id":"CsWA-mGEqwnE"},"source":["\n","\n","---\n","\n","\n","### Write here the names of the group members\n","\n","**Name 1**: \n","\n","**Name 2**:\n","\n","You can answer in Catalan, Spanish or English\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jSM5HeJrGav8"},"source":["## 1. Motivation\n","\n","The objective of this session is to analyze the main characteristics of the audiovisual signals and the main concepts of a coding system.\n","\n","We will be using the some additional images and videos. First of all download them from ATENEA and upload them to the Colaboratory notebook using the menu on the left (click the folder icon, upload the cm_l1.zip file). Wait until the upload is complete, it may take some time.\n","\n","*Decompress* the zip file to use its contents:`\n"]},{"cell_type":"code","metadata":{"id":"fvaapC1_TFxs"},"source":["!unzip cm_lab1.zip \n","!rm cm_lab1.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6bW0fWPS4J8"},"source":["Consider a network with a speed of 6Mbps. We want to stream audio and video. The audio is de CD quality (sampling frequency 44100 Hz, 16 bits/sample, 2 channels), which results in a bitrate of:"]},{"cell_type":"code","metadata":{"id":"uuYkOeIZHTET"},"source":["audio_rate = 44100 * 16 * 2\n","print('Bitrate = {} bits/s'.format(audio_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R13VA9_tHkEX"},"source":["For this audio rate, the maximum video resolution (area) that can be obtained is 7640 pixels, for a frame-rate of 25 fps. This value is obtained knowing that each frame has 3 channels, and each channel is made up of pixels values range from 0 to 255. You can check this with the following calculations:\n"]},{"cell_type":"code","metadata":{"id":"pRMUqd4HHyuU"},"source":["resolution = 7640;\n","video_rate= 25*resolution*3*8\n","total_rate = audio_rate + video_rate\n","print('Total bitrate = {} bits/s ~ 6 Mbps'.format(total_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v9l3pxD4IKmY"},"source":["If we have a display with a square aspect ratio, 7640 pixels correspond to an image of about 87x87 pixels. In a 16x9 aspect ratio display, this results in a 117x65 pixel image.\n","\n","These resulutions will appear tiny in a modern computer monitor, with resolutions up to 1960x1080 or larger. \n"]},{"cell_type":"markdown","metadata":{"id":"5b6J42u-IuLG"},"source":["If we want to improve the resolution and therefore improve the quality, we will need to compress the data. To do this, the usual scheme consists of three blocks: signal space transformation, quantification, and entropic coding."]},{"cell_type":"markdown","metadata":{"id":"zZHZApmlJIEh"},"source":["First, we will analyze some properties that characterize image and audio signals. We will then take advantage of these properties using the proposed coding scheme."]},{"cell_type":"markdown","metadata":{"id":"kp5iKtXTJRlt"},"source":["## 2. Signal model\n","### 2.1 Image\n","#### 2.1.1 Spatial redundancy\n","\n","In the spatial domain, there is usually a high correlation between pixels (samples) that are close. That is, the values of neighboring samples are usually similar to each other. To illustrate this property, upload the image baboon.jpg."]},{"cell_type":"code","metadata":{"id":"sjzvWvBWJ6eT"},"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from lab1.code.display_images import display_image, display_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s91BrqfK2IBn"},"source":["baboon = cv2.imread('lab1/images/baboon.jpg')\n","baboon_gray = cv2.cvtColor(baboon, cv2.COLOR_BGR2GRAY) # convert to gray\n","\n","display_image (baboon_gray, 'Baboon', size = 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JODMXsBVLP68"},"source":["Compute the correlation coefficient between adjacent pairs of pixels:"]},{"cell_type":"code","metadata":{"id":"s5_tV9cTLR4U"},"source":["dbaboon  = baboon_gray.astype(float).flatten()\n","corr_coef = np.corrcoef(dbaboon[0:-1],dbaboon[1:]) \n","print ('Correlation coefficient = {:.3f}'.format(corr_coef[0][1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_yx7t0XLr8h"},"source":["\n","Now calculate the 2D histogram of the adjacent pairs of pixels, and comment on the result:"]},{"cell_type":"code","metadata":{"id":"t6PDoJctLwYY"},"source":["xedges = np.linspace(0,250,251) \n","yedges = np.linspace(0,250,251)\n","histmat, xedges, yedges = np.histogram2d(dbaboon[1:], dbaboon[0:-1], bins=(xedges, yedges))\n","plt.imshow(histmat, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","plt.xlabel('Amplitude of current pixel') \n","plt.ylabel('Amplitude of adjacent pixel')\n","plt.title('Adjacent pixels 2D Histogram')  \n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PluKLBML8kW"},"source":["Comment the results (histogram of adjacent pixels and correlation coefficient):\n","\n","<font color=red>**Q1 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Alky2801MFKM"},"source":["#### 2.1.2. Inter-channel redundancy\n","\n","A true color image is represented using three channels.\n","Usually, when working with multispectral images, there is a strong correlation between channels. This can be clearly seen in the Baboon image and its RGB components:"]},{"cell_type":"code","metadata":{"id":"mJzoY-HLaWOa"},"source":["baboon_rgb  = cv2.cvtColor(baboon, cv2.COLOR_BGR2RGB)   # Convert to RGB\n","display_images(baboon_rgb,    baboon[:,:,0], 'Baboon',    'R Channel', size = 0.5)\n","display_images(baboon[:,:,1], baboon[:,:,2], 'G Channel', 'B Channel', size = 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tz_1y_y6a5AM"},"source":["A common technique for analyzing the redundancy shared between different channels is to compare each pixel in a pair of channels (R-channel and G-channel, for example). We first calculate the correlation coefficient between corresponding pixels (in the same position) in the two channels:"]},{"cell_type":"code","metadata":{"id":"QItTmCCUbGaj"},"source":["corr_coef = np.corrcoef(baboon_rgb[:,:,0].flatten().astype(float), baboon_rgb[:,:,1].flatten().astype(float))\n","print ('Correlation coefficient = {:.3f}'.format(corr_coef[0][1])) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7uDOjd5bgrx"},"source":["Now compute the 2D histogram of the corresponding pixels and\n","comment on the results:"]},{"cell_type":"code","metadata":{"id":"WLQqY9prbrYs"},"source":["xedges = np.linspace(0,255,256)\n","yedges = np.linspace(0,255,256)\n","fbaboon_rgb = baboon_rgb.astype(float)\n","histmat, xedges, yedges = np.histogram2d(fbaboon_rgb[:,:,0].flatten(), fbaboon_rgb[:,:,1].flatten(), bins=(xedges, yedges))\n","plt.imshow(histmat, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","plt.xlabel('R channel')\n","plt.ylabel('G channel'); \n","plt.title('R-G channels 2D Histogram')  \n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnOv6Xxubvv_"},"source":["<font color=red>**Q2 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font> "]},{"cell_type":"markdown","metadata":{"id":"Bypk-KLkb9y2"},"source":["Consider the same image in another color space (YCbCr) and perform the same operation as before:\n"]},{"cell_type":"code","metadata":{"id":"r97RZ7BZcJUE"},"source":["baboon_ycrcb = cv2.cvtColor(baboon, cv2.COLOR_BGR2YCR_CB)\n","\n","display_images(baboon_rgb, baboon_ycrcb[:,:,0], 'Color image', 'Y Channel', size=0.5)\n","display_images(baboon_ycrcb[:,:,2], baboon_ycrcb[:,:,1], 'Cb channel', 'Cr Channel', size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvVBNW-b5bST"},"source":["Compute the correlation coefficient:"]},{"cell_type":"code","metadata":{"id":"3JISQ4oc5xji"},"source":["corr_coef_YCb = np.corrcoef(baboon_ycrcb[:,:,0].flatten().astype(float), baboon_ycrcb[:,:,2].flatten().astype(float))\n","corr_coef_YCr = np.corrcoef(baboon_ycrcb[:,:,0].flatten().astype(float), baboon_ycrcb[:,:,1].flatten().astype(float))\n","\n","print ('Correlation coefficient Y-Cb = {:.3f}'.format(corr_coef_YCb[0][1]))\n","print ('Correlation coefficient Y-Cr = {:.3f}'.format(corr_coef_YCr[0][1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YjI3pM5f67e1"},"source":["Compute the 2d histograms of chanels Y and Cb and channels Y and Cr:"]},{"cell_type":"code","metadata":{"id":"FtCsZR0U7HmG"},"source":["fbaboon_ycrcb = baboon_ycrcb.astype(float)\n","histmat1, xedges, yedges = np.histogram2d(fbaboon_ycrcb[:,:,0].flatten(), fbaboon_ycrcb[:,:,2].flatten(), bins=(xedges, yedges))\n","histmat2, xedges, yedges = np.histogram2d(fbaboon_ycrcb[:,:,0].flatten(), fbaboon_ycrcb[:,:,1].flatten(), bins=(xedges, yedges))\n","\n","fig, ax = plt.subplots(1,2)\n","plt.grid(False)\n","h = ax[0].imshow(histmat1, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","\n","hsep = 0.1\n","size = 1.0\n","dpi = h.figure.get_dpi()/size\n","h.figure.set_figwidth(histmat1.shape[1] / dpi)\n","h.figure.set_figheight(histmat1.shape[0] / dpi)\n","h.figure.canvas.resize(histmat1.shape[1] + 1, histmat1.shape[0] + 1)\n","h.axes.set_position([0, 0, 1, 1])\n","\n","ax[0].set_xlabel('Y channel')\n","ax[0].set_ylabel('Cb channel'); \n","ax[0].set_title('Y-Cb channels 2D Histogram')  \n","h = ax[1].imshow(histmat2, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","dpi = h.figure.get_dpi()/size\n","\n","h.figure.set_figwidth(histmat2.shape[1] / dpi)\n","h.figure.set_figheight(histmat2.shape[0] / dpi)\n","h.figure.canvas.resize(histmat2.shape[1] + 1, histmat2.shape[0] + 1)\n","h.axes.set_position([1+hsep, 0, 1, 1])\n","\n","ax[1].set_xlabel('Y channel')\n","ax[1].set_ylabel('Cr channel'); \n","ax[1].set_title('Y-Cr channels 2D Histogram')  \n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIcrPqxg-_1E"},"source":["Discuss the differences between the correlations of the pixels in both color spaces. In which **color space** are the channels most correlated?\n","\n","<font color=red>**Q3 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"iX3RwAfq_fZx"},"source":["However, if we compare the correlations between the chrominance channels (Cb and Cr) of the YCbCr image we can observe a strong correspondence between components. First calculate the correlation coefficient:"]},{"cell_type":"code","metadata":{"id":"GPDsRCOO_kVx"},"source":["corr_coef_CbCr = np.corrcoef(baboon_ycrcb[:,:,2].flatten().astype(float), baboon_ycrcb[:,:,1].flatten().astype(float))\n","print ('Correlation coefficient Cb-Cb = {:.3f}'.format(corr_coef_CbCr[0][1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUItjJTh_tOP"},"source":["Now calculate the 2D histogram of the Cb and Cr channels:"]},{"cell_type":"code","metadata":{"id":"bwFPgD2y_yJa"},"source":["histmat3, xedges, yedges = np.histogram2d(fbaboon_ycrcb[:,:,2].flatten(), fbaboon_ycrcb[:,:,1].flatten(), bins=(xedges, yedges))\n","\n","plt.imshow(histmat3, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","plt.xlabel('Cb channel')\n","plt.ylabel('Cr channel'); \n","plt.title('Cr-Cb channels 2D Histogram')  \n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_ppXpC4AhZk"},"source":["Despite the fact that the chrominance channels are not independent of each other, the color space transformation is equally useful. Why?\n","Before answering this question, it is better to solve subsection 2.1.4\n","\n","<font color=red>**Q4 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"5_r4Hhp0Fucy"},"source":["#### 2.1.3. Temporal redundancy\n","\n","When working with videos, frames that are temporarily close share a lot of information. In particular, the values of the pixels that are in the same position in consecutive frames show a high correlation. In this section we will analyze spatial redundancy using differences between frames.\n","The Frame Difference between two images I (n) and I (n + a) is calculated as the difference between the two images:\n","```\n","Df[I(n),I(n+1)] = |I(n)-I(n+1)|\n","```"]},{"cell_type":"code","metadata":{"id":"FxIX4u1uGL1s"},"source":["im1 = cv2.imread('lab1/images/00002979.jpg')\n","im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n","\n","im2 = cv2.imread('lab1/images/00002981.jpg')\n","im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n","\n","display_images (im1, im2, 'Frame n', 'Frame n+2', size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR-77hEYHy59"},"source":["frame_dif = np.abs(im1.astype(int)-im2.astype(int)).astype(np.uint8)\n","display_image(frame_dif, 'Frame Difference', size=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coEPVmJlHj-i"},"source":["Discuss the meaning of the resulting Frame Difference in terms of temporal redundancy. What is the meaning of the white areas and the black areas? Include the concept of correlation in your answer.\n","\n","\n","<font color=red>**Q5 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"vSyZS45mIQeM"},"source":["#### 2.1.4. Irrelevancy\n","\n","The concept of irrelevance refers to the presence of certain information in the image that is not perceptually relevant to the human visual system. Consequently, the removal of this information is generally imperceptible. To analyze this concept, we will remove one out of every two chrominance samples (one out of every two rows) from an image rendered in YCbCr space. Then we will reconstruct the image by interpolating the chrominance channels through row replication.\n"]},{"cell_type":"code","metadata":{"id":"7FBWQSfiI6X3"},"source":["from lab1.code.psnr_mse_maxerr import psnr_mse_maxerr \n","\n","im = cv2.imread('lab1/images/lenna.png')\n","im_rgb  = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # Convert to RGB\n","\n","im_yvu  = cv2.cvtColor(im, cv2.COLOR_BGR2YCR_CB)\n","im_yvu2 = im_yvu.copy()\n","\n","height  = im_yvu.shape[0]\n","\n","display_images(im_rgb, im_yvu[:,:,0], 'Color image', 'Y channel', size=0.5)\n","display_images(im_yvu[:,:,2], im_yvu[:,:,1], 'U channel', 'V channel', size=0.5)\n","\n","print ('\\nDownsample chroma channels\\n')\n","display_image(im_yvu[:,:,0], 'Y channel', size=0.5)\n","display_images(im_yvu[range(0,height,2),:,2], im_yvu[range(0,height,2),:,1], 'Downsampled U channel', 'Downsampled V channel', size=0.5)\n","\n","print ('\\nReconstruct from downsampled chroma channels\\n')\n","im_yvu[range(1,height,2),:,1:3] = im_yvu[range(0,height,2),:,1:3]  \n","\n","display_images(im_rgb, cv2.cvtColor(im_yvu, cv2.COLOR_YCR_CB2RGB), 'Original', 'Reconstructed', size=0.5)\n","psnr,mse,_,_ = psnr_mse_maxerr(im_rgb, cv2.cvtColor(im_yvu, cv2.COLOR_YCR_CB2RGB)) \n","print ('PSNR = {}, MSE = {}'.format(psnr,mse))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0HLNSPyiJwa5"},"source":["In the figure you can see the original and reconstructed images. What is the reduction obtained in terms of the number of samples? Do you think this reduction is proportional to the error observed between the two images? Consider the PSNR values. Is the PSNR a good measure of quality when working with images?\n","\n","<font color=red>**Q6 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>\n"]},{"cell_type":"markdown","metadata":{"id":"WDZvjSFIJQl2"},"source":["### 2.2 Audio\n","#### 2.2.1 Temporal redundancy\n","\n","In the temporal domain, the audio signals are strongly correlated. To illustrate this concept, we will present two windows of two different signals: the first is of a tenor singing and the second is of a flute."]},{"cell_type":"code","metadata":{"id":"m7OjXGMDXBYc"},"source":["!pip install soundfile   # Install necessary packages"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XV2rEgs0z-da"},"source":["from lab1.code.audio_functions import show_signals\n","import soundfile as sf\n","\n","tenor, fs = sf.read('lab1/audio/tenor.wav')\n","flute, fs = sf.read('lab1/audio/flute.wav')\n","\n","corr_coef_tenor, corr_coef_flute = show_signals(tenor, flute, 'Tenor', 'Flute', fs)\n","plt.tight_layout()\n","\n","print ('Correlation coefficient tenor = {}'.format(corr_coef_tenor))\n","print ('Correlation coefficient flute = {}'.format(corr_coef_flute))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0KBH3TNz-u1"},"source":["The two results are the autocorrelation values normalized to 1. Discuss the relationship between these values and the plots of the correlations (the center graphs).\n","\n","<font color=red>**Q7 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"AgKEq3-YIdNb"},"source":["The previous example only analyzed a narrow window in time (20ms) in which the signal can be considered stationary. However, the audio signals vary very quickly and the different sounds appear concatenated. To exemplify this phenomenon, we will observe a voice signal where a female voice pronounces various sounds."]},{"cell_type":"code","metadata":{"id":"xAaVO_xFIsoS"},"source":["from lab1.code.audio_functions import show_speech\n","from IPython.display import Audio\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')\n","speech_part = speech[44000-1:100000,0]    \n","\n","show_speech(speech_part,fs)\n","\n","Audio(data=speech_part, rate=fs)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kha1CfIZI2wg"},"source":["In this example we can clearly see two different classes of spectra: those with high content in low frequencies and those with higher content in high frequencies. They correspond to voiced or unvoiced sounds.\n","Can you identify them?\n","\n","<font color=red>**Q8 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"b6qDnalGILI4"},"source":["#### 2.2.2 Masking\n","\n","Masking occurs when the perception of a sound is affected by the presence of another louder sound that is close in frequency. This effect is due to the internal mechanism of the ear. The different sounds are translated into vibrations in specific positions of the cochlea, and if two sounds vibrate in close positions, the loudest is the one that predominates.\n","You can reproduce this effect with a synthetic sound. Listen to the following signal and comment if you can hear one or more sounds.\n","Compare your perception with the spectrogram shown in the figure.\n"]},{"cell_type":"code","metadata":{"id":"2QsCIRw1TIuD"},"source":["from lab1.code.audio_functions import play_masked_tone\n","\n","masked_tone, fs = play_masked_tone()\n","Audio(data=masked_tone, rate=fs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ5TOQdvTSKD"},"source":["Compare your perception with the spectrogram shown in the figure:\n","\n","<font color=red>**Q9 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>\n"]},{"cell_type":"markdown","metadata":{"id":"faJIivDjTTxZ"},"source":["## 3. Transform or prediction block\n","\n","After analyzing the main characteristics of image and audio signals, we will study how we can compress these signals taking advantage of these characteristics.\n","\n","### 3.1 Transform\n","#### 3.1.1 Image\n","\n","The Transform block changes the description of the signal into a less correlated set of transformed samples that describe the same signal. It transforms the signal into a format designed to reduce its redundancy. Usually the transformation is reversible, but the operation itself does not reduce the amount of data that must be transmitted.\n","\n","Now we apply a transformation to two different blocks of the same image in gray levels (lenna_gray) and observe the images:\n"]},{"cell_type":"code","metadata":{"id":"QFX1KVpBOroG"},"source":["import numpy as np\n","from scipy.fftpack import dct\n","from scipy import signal\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","lenna      = cv2.imread('lab1/images/lenna.png')\n","lenna_gray = cv2.cvtColor(lenna, cv2.COLOR_BGR2GRAY) # convert to gray\n","dlenna     = lenna_gray.astype(float)\n","block_1    = dlenna[49:81,429:461]\n","block_2    = dlenna[249:281,299:331]\n","\n","B_1 = dct(dct(block_1,axis=0,norm='ortho'),axis=1,norm='ortho')\n","B_2 = dct(dct(block_2,axis=0,norm='ortho'),axis=1,norm='ortho')\n","B_1v = np.round(np.clip(B_1, 0,255)).astype(np.uint8)\n","B_2v = np.round(np.clip(B_2, 0,255)).astype(np.uint8)\n","\n","fig, ax = plt.subplots(2,2)\n","h = ax[0][0].imshow(block_1.astype(np.uint8),cmap='gray')\n","ax[0][0].set_title('Block 1')\n","ax[0][0].axis('off')\n","\n","h = ax[0][1].imshow(B_1v.astype(np.uint8), cmap='gray')\n","ax[0][1].set_title('Transformed Block 1')\n","ax[0][1].axis('off')\n","\n","h = ax[1][0].imshow(block_2.astype(np.uint8), cmap='gray')\n","ax[1][0].set_title('Block 2')\n","ax[1][0].axis('off')\n","\n","h = ax[1][1].imshow(B_2v.astype(np.uint8),cmap='gray')\n","ax[1][1].set_title('Transformed Block 2')\n","ax[1][1].axis('off')\n","\n","plt.tight_layout()\n","plt.show() \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wqrnyxDOsEL"},"source":["What can you say about the structure of the transformed coefficients? Is it suitable for encoding?\n","\n","<font color=red>**Q10 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"bwPD402fO31g"},"source":["Also note the autocorrelations of the original and transformed blocks, and comment on the differences in terms of spatial redundancy."]},{"cell_type":"code","metadata":{"id":"E7P8vfJcO8ST"},"source":["fig, ax = plt.subplots(2,2)\n","ax[0][0].contourf(range(-31,32), range(-31,32), signal.correlate2d(block_1,block_1),100, cmap='jet')\n","ax[0][0].set_title('Autocorrelation Block 1')\n","ax[0][0].axis('equal')\n","\n","ax[0][1].contourf(range(-31,32), range(-31,32), signal.correlate2d(B_1,B_1),100, cmap='jet')\n","ax[0][1].set_title('Autocorrelation Transformed Block 1')\n","ax[0][1].axis('equal')\n","ax[1][0].contourf(range(-31,32), range(-31,32), signal.correlate2d(block_2, block_2),100, cmap='jet')\n","ax[1][0].set_title('Autocorrelation Block 2')\n","ax[1][0].axis('equal')\n","ax[1][1].contourf(range(-31,32), range(-31,32), signal.correlate2d(B_2, B_2),100, cmap='jet')\n","ax[1][1].set_title('Autocorrelation Transformed Block 2')\n","ax[1][1].axis('equal')\n","\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvyKQNKEO8gK"},"source":["<font color=red>**Q11 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"4KXLW05fPGKa"},"source":["#### 3.1.2 Audio\n","In order to exploit the benefits of the transformation, the audio signals must be divided into short windows, where they can be considered stationary. We will analyze a particular transformation (the discrete cosine transformed DCT) into a speech signal with 20ms windows."]},{"cell_type":"code","metadata":{"id":"FhygnbY9PUaM"},"source":["from lab1.code.audio_functions import transform_speech\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')\n","speech     = speech[44000-1:100000,1] \n","\n","transform_speech(speech,fs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_aFelCTzPdOa"},"source":["The figure shows a transformed signal and two portions of the signal, with the corresponding magnitudes of the transformed coefficients.\n","Can you tell what kinds of portions of the signal have been transformed, voiced or unvoiced? What can you say about the structure of the significant coefficients? Is it suitable for encoding?\n","\n","<font color=red>**Q12 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"kbzQU-vLmBf5"},"source":["### 3.2 Prediction\n","#### 3.2.1 Image\n","\n","Instead of transforming a signal, another option is to use a predictive model in which we take advantage of spatial redundancy. For example, we can predict each pixel in the image by averaging some of its neighboring pixels (in the example, three pixels from the previous row and the previous pixel from the current row).\n","\n","![image2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHMAAABzCAIAAAAkIaqxAAAAAXNSR0IArs4c6QAAAAlwSFlzAAAOwwAADsMBx2+oZAAAAnVJREFUeF7t2+tugkAUReHSJ/fNLYaEmqJymbOcTV39rXvox84UJj3D9Xr98gcQ+AYyjbwJKEv1YJh3g2EYqEU+KffX8172crm8B2Fc6L+uNXu6G1BdUlZZSoDKtbPKUgJUrp1VlhKgcu2sspQAlWtnlaUEqFw7qywlQOXaWWUpASrXzipLCVC5dlZZSoDKtbPKUgJUrp1VlhKgcu2sspQAlWtnlaUEqFw7qywlQOXaWWUpASrXzipLCVC5dpaSdU6hWLb/nMLb5tDGyZYua7kbFHd2jguQJaaliMydt6C37ERQC0Fk7mQdP95bdr7iKtyqnP2Uf77RW/Z+nrod5T6h96R2b9nxRlfhJrHG7AbtuGGsMbKNzc1jTZI9jBvJGiZ7ADeVNU92F24wa6TsRtxs1lTZVdx41mDZF7hnYM2WfYh7EtZ42RfN7f3yunquEPB2u3qNS8R41jN0dtU99QNn6OzyDKz9VIy/H/Gyz/5kxeNmyy5Z20/F+LZOKwTLPmvrSXBTZV8/t54BN1J2y+tAPG6e7BbWaSfLxg2T3c4aj5sku5c1GzdG9hhrMG6GbAtrKm6AbDtrJG5v2SrWPNzesvOTU9XBYHng0bfh3rLTY2kV69zc2sBDuAGyh647/0vOKRTfo8dzCl3+n7/4N1vEOadAC787332WEldWWUqAyrWzylICVK6dVZYSoHLtrLKUAJVrZ5WlBKhcO6ssJUDl2lllKQEq184qSwlQuXZWWUqAyrWzylICVK6dVZYSoHLtrLKUAJVrZ5WlBKhcO6ssJUDl2lllKQEq184qSwlQuc4pFMs+mFMoXuHj49xnqQooS8n+AGv/vcPzsm1EAAAAAElFTkSuQmCC)"]},{"cell_type":"markdown","metadata":{"id":"gDd3EnDVmfMw"},"source":["Why don't we use the blank pixels in the figure to predict pixel X?\n","\n","<font color=red>**Q13 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"p2RII67lmlR9"},"source":["Thus, it is only necessary to send the prediction error, that is, the difference between the original value and the prediction value."]},{"cell_type":"code","metadata":{"id":"7MuIhFZQmudl"},"source":["from lab1.code.predictive_models import predictive_model_image\n","\n","lenna      = cv2.imread('lab1/images/lenna.png')\n","lenna_gray = cv2.cvtColor(lenna, cv2.COLOR_BGR2GRAY).astype(float) # convert to gray                                                    \n","\n","predictive_model_image(lenna_gray)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BG6iPR-89_ZS"},"source":["What kind of redundancy is exploited with the previous predictive model example? In which parts of the image are the most significant errors concentrated? Why?\n","\n","<font color=red>**Q14 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"qMgLO6lw-DCl"},"source":["Let's consider an encoding scheme. If a predictive model is used, what information must the transmitter send to the receiver? What is the procedure that the receiver must follow to recover the original image?\n","\n","<font color=red>**Q15 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"-LZev8Kt-Pgy"},"source":["#### 3.2.2 Audio\n","\n","A simple model for predicting audio signals is to consider only the previous sample, and transmit only the prediction error \n","```\n","e (n) = s(n) - s(n-1))\n","```"]},{"cell_type":"code","metadata":{"id":"oytYWtrQ-gSH"},"source":["from lab1.code.predictive_models import predictive_model_speech\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')                                                                                                 \n","speech     = speech[44000-1:100000,1]    \n","predictive_model_speech (speech, fs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"epNAckk5-jdk"},"source":["In this scheme, there are times when the errors are large, and others, regardless of the amplitude of the signal, when they are negligible.\n","From looking at the figure, can you identify the reason? Is there a relationship with the voiced / unvoiced characteristics of phonemes?\n","\n","<font color=red>**Q16 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"eDwz-bmDEJkK"},"source":["## 4. Quantization block\n","### 4.1 Image\n","\n","This block reduces the precision of the transform block output according to a pre-set fidelity criterion. The goal is to remove irrelevant information from the compressed representation.\n","\n","The quantifier represents a range of values ​​from a sample transformed by a single value in the range. The set of output values ​​are the quantization indices. This step is responsible for the losses in the system and therefore determines the degree of compression achieved (here we are assuming scalar quantization, but it is also possible to group a set of transformed samples into a vector and perform vector quantization).\n","\n","This operation is irreversible, and therefore should be omitted when lossless compression is desired.\n","\n","Next, we will perform a quantization on a block of the image using different quantization steps. \n"]},{"cell_type":"code","metadata":{"id":"LO3EMcCnt-rA"},"source":["import cv2\n","from scipy.fftpack import dct\n","from skimage.measure import shannon_entropy\n","import numpy as np\n","\n","lenna      = cv2.imread('lab1/images/lenna.png')\n","lenna_gray = cv2.cvtColor(lenna, cv2.COLOR_BGR2GRAY) # convert to gray\n","dlenna     = lenna_gray.astype(float)\n","\n","block = dlenna[129:161,99:131]\n","B = dct(dct(block,axis=0,norm='ortho'),axis=1,norm='ortho')\n","\n","kk   = 0\n","err  = np.zeros(1901)\n","ent  = np.zeros(1901)\n","\n","for ii in np.linspace(1,20,1901):\n","    Bq = np.round(B/ii)*ii\n","    err[kk] = np.sum(np.abs(B-Bq)**2)\n","    ent[kk] = shannon_entropy(Bq)\n","    kk = kk + 1\n","\n","plt.plot(ent,10*np.log10(err))\n","plt.xlabel('Entropy')\n","plt.ylabel('MSE [dB]')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JIo0EmumqWV4"},"source":["The figure presents the relationship between error and entropy.\n","\n","The purpose of the figure is to show the impact of the quantizer on the relationship between compression rate and distortion (rate and distortion). However, since the compression calculation involves one encoding step, the compression rate will be approximated by entropy."]},{"cell_type":"markdown","metadata":{"id":"2QRUoTXvq8wA"},"source":["Discuss the relationship between the entropy of the signal and the compression rate that can be obtained by encoding it.\n","\n","<font color=red>**Q17 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"b8d05jlVt4_u"},"source":["Discuss the relationship between quantization levels and the entropy of the output signal relative to the entropy of the input signal in the quantization step.\n","\n","<font color=red>**Q18 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Psz34iCHJ7TB"},"source":["Discuss the relationship between compression rate and distortion (error) depending on the number of quantization levels. Hint: Remember that, quantitatively, the compression rate can be approximated by the entropy of the signal.\n","\n","<font color=red>**Q19 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Y7nAAfS1KG4t"},"source":["Why do you think it is important to get a small entropy value? Is it possible to achieve an arbitrarily small compression rate? Why?\n","\n","<font color=red>**Q20 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"flMuSxflKOSC"},"source":["### 4.2 Audio\n","\n","To demonstrate the effects of audio quantization, we will examine two different types of quantization. The first is the quantization of the signal itself with two different types of quantizer: one uniform and the other non-uniform. The L parameter is the quantization step and should be adjusted to vary the quality (start with L = 0.01 and change the value)."]},{"cell_type":"code","metadata":{"id":"VvRQO5cuKa-y"},"source":["from lab1.code.audio_functions import mulaw\n","import soundfile as sf\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')                                                                                                 \n","speech     = speech[44000-1:100000,1]  \n","\n","L = 0.01\n","sq, imu = mulaw(speech, fs, L)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIBKddVtt4I5"},"source":["Audio(data=sq,  rate=fs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKgdsa_6t6IH"},"source":["Audio(data=imu, rate=fs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VPJRBOTKemF"},"source":["Which one sounds better? Note that the **imu** signal has been logarithm-transformed before quantization. Can this transformation affect perception? (Hint: think about what happens to small samples and ear sensitivity)\n","\n","\n","<font color=red>**Q21 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"0gSf_8njK0v_"},"source":["The second type of quantization will be the quantization of the prediction error. Only a uniform quantizer will be applied here. Adjust the L parameter."]},{"cell_type":"code","metadata":{"id":"XKVTVSLYLAGo"},"source":["from lab1.code.audio_functions import quantize_error\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')                                                                                                 \n","speech     = speech[44000-1:100000,0] \n","\n","L=0.01\n","rs, fs = quantize_error(speech, fs, L)\n","Audio(data=rs,  rate=fs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o36GVGcSLFLl"},"source":["Is the quality of this signal comparable to either of the two above?\n","\n","<font color=red>**Q22 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"2TdIRvj1LIUs"},"source":["## 5. Entropy coding\n","\n","This block produces the final bistream. The entropic encoder generates a fixed or variable length code to represent the output of the quantizer, and encodes the output according to the code. In many cases a variable length code is used: the shortest code words are assigned to the most frequent output values, minimizing code redundancy. This operation is reversible.\n","\n","Next, we have generated two signals that come from two different sources. One of the sources has a Gaussian distribution, while the other has a uniform distribution. Both sources can generate nine different symbols. Then the symbols have been encoded using a Huffman encoder."]},{"cell_type":"code","metadata":{"id":"S998LMdXcR2v"},"source":["!pip install huffman # Install necesssary package"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3DD6sJ9WLc30"},"source":["import pickle\n","import huffman\n","\n","with open('lab1/random_signals.pkl', 'rb') as fd:\n","  rd = pickle.load(fd)\n","n1 = rd['n1'].squeeze()\n","n2 = rd['n2'].squeeze()\n","\n","symbols = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4]) # Distinct data symbols appearing\n","x1,_ = np.histogram(n1,9)\n","x2,_ = np.histogram(n2,9)\n","N = n1.shape[0]  \n","p1 = x1/N\n","p2 = x2/N\n","dict1 = huffman.codebook(zip(symbols,p1))\n","dict2 = huffman.codebook(zip(symbols,p2))\n","\n","hcode1 = ''.join([dict1[ii] for ii in n1]) # Encode the data.\n","hcode2 = ''.join([dict2[ii] for ii in n2]) # Encode the data.\n"," \n","length1 = len(hcode1)\n","length2 = len(hcode2) \n","\n","entropy1 =  -np.sum(p1 * np.log2(p1))  \n","average_length_1 = len(hcode1)/N  \n","\n","entropy2 = -np.sum(p2 * np.log2(p2))  \n","average_length_2 = len(hcode2)/N  \n","\n","print ('Entropy 1 = {:.2f}, Average length 1 = {:.2f}'.format(entropy1, average_length_1))\n","print ('Entropy 2 = {:.2f}, Average length 2 = {:.2f}'.format(entropy2, average_length_2))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZErL51gLLh1f"},"source":["From the analysis of the entropy values, identify which signal corresponds to the Gaussian distribution and which to the uniform distribution. Also compare the entropy values with the mean length. Do these values satisfy the condition \n","```\n","H(x) <= L <= H(x)+1?\n","```\n","<font color=red>**Q23 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]}]}