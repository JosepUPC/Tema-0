{"cells":[{"cell_type":"markdown","metadata":{"id":"Dn2cLC7eF8uo"},"source":["# **ESEIAAT - Codificació Multimèdia**\n","## **Lab 1: Intro**\n","\n","2021 - Josep Ramon Morros - [GPI](https://imatge.upc.edu/web/) @ [IDEAI](https://ideai.upc.edu/en) Research group // ESEIAAT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Fzdlb9rm499"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"CsWA-mGEqwnE"},"source":["\n","\n","---\n","\n","\n","### Write here the names of the group members\n","\n","**Name 1**: Josep Esquerrà Bayo\n","\n","**Name 2**:\n","\n","You can answer in Catalan, Spanish or English\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jSM5HeJrGav8"},"source":["## 1. Motivation\n","\n","The objective of this session is to analyze the main characteristics of the audiovisual signals and the main concepts of a coding system.\n","\n","We will be using the some additional images and videos. First of all download them from ATENEA and upload them to the Colaboratory notebook using the menu on the left (click the folder icon, upload the cm_l1.zip file). Wait until the upload is complete, it may take some time.\n","\n","*Decompress* the zip file to use its contents:`\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"fvaapC1_TFxs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708877087044,"user_tz":-60,"elapsed":323,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"469c2c75-976a-4f0e-b087-1d3dce69adbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  cm_lab1.zip\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of cm_lab1.zip or\n","        cm_lab1.zip.zip, and cannot find cm_lab1.zip.ZIP, period.\n"]}],"source":["!unzip cm_lab1.zip\n","!rm cm_lab1.zip"]},{"cell_type":"markdown","metadata":{"id":"d6bW0fWPS4J8"},"source":["Consider a network with a speed of 6Mbps. We want to stream audio and video. The audio is de CD quality (sampling frequency 44100 Hz, 16 bits/sample, 2 channels), which results in a bitrate of:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"uuYkOeIZHTET","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708877087044,"user_tz":-60,"elapsed":3,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"8d761951-0930-4c67-fe76-3df834d594f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Bitrate = 1411200 bits/s\n"]}],"source":["audio_rate = 44100 * 16 * 2\n","print('Bitrate = {} bits/s'.format(audio_rate))"]},{"cell_type":"markdown","metadata":{"id":"R13VA9_tHkEX"},"source":["For this audio rate, the maximum video resolution (area) that can be obtained is 7640 pixels, for a frame-rate of 25 fps. This value is obtained knowing that each frame has 3 channels, and each channel is made up of pixels values range from 0 to 255. You can check this with the following calculations:\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pRMUqd4HHyuU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708877087044,"user_tz":-60,"elapsed":2,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"c8565993-e993-48c4-f1c1-ddb5a7cbbbff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total bitrate = 5995200 bits/s ~ 6 Mbps\n"]}],"source":["resolution = 7640;\n","video_rate= 25*resolution*3*8\n","total_rate = audio_rate + video_rate\n","print('Total bitrate = {} bits/s ~ 6 Mbps'.format(total_rate))"]},{"cell_type":"markdown","metadata":{"id":"v9l3pxD4IKmY"},"source":["If we have a display with a square aspect ratio, 7640 pixels correspond to an image of about 87x87 pixels. In a 16x9 aspect ratio display, this results in a 117x65 pixel image.\n","\n","These resulutions will appear tiny in a modern computer monitor, with resolutions up to 1960x1080 or larger.\n"]},{"cell_type":"markdown","metadata":{"id":"5b6J42u-IuLG"},"source":["If we want to improve the resolution and therefore improve the quality, we will need to compress the data. To do this, the usual scheme consists of three blocks: signal space transformation, quantification, and entropic coding."]},{"cell_type":"markdown","metadata":{"id":"zZHZApmlJIEh"},"source":["First, we will analyze some properties that characterize image and audio signals. We will then take advantage of these properties using the proposed coding scheme."]},{"cell_type":"markdown","metadata":{"id":"kp5iKtXTJRlt"},"source":["## 2. Signal model\n","### 2.1 Image\n","#### 2.1.1 Spatial redundancy\n","\n","In the spatial domain, there is usually a high correlation between pixels (samples) that are close. That is, the values of neighboring samples are usually similar to each other. To illustrate this property, upload the image baboon.jpg."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"sjzvWvBWJ6eT","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"error","timestamp":1708877515998,"user_tz":-60,"elapsed":261,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"94948dcd-b5c6-470a-be11-be6e55b8576e"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'lab1'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-c421e18f582c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlab1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_images\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab1'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from lab1.code.display_images import display_image, display_images"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"s91BrqfK2IBn","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1708877097668,"user_tz":-60,"elapsed":379,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"5b956882-4cae-471e-cee5-4405d98f5c3e"},"outputs":[{"output_type":"error","ename":"error","evalue":"OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-eb0c74aa3782>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbaboon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lab1/images/baboon.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbaboon_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaboon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert to gray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay_image\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbaboon_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Baboon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["baboon = cv2.imread('lab1/images/baboon.jpg')\n","baboon_gray = cv2.cvtColor(baboon, cv2.COLOR_BGR2GRAY) # convert to gray\n","\n","display_image (baboon_gray, 'Baboon', size = 0.5)"]},{"cell_type":"markdown","metadata":{"id":"JODMXsBVLP68"},"source":["Compute the correlation coefficient between adjacent pairs of pixels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5_tV9cTLR4U","executionInfo":{"status":"aborted","timestamp":1708877097668,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["dbaboon  = baboon_gray.astype(float).flatten()\n","corr_coef = np.corrcoef(dbaboon[0:-1],dbaboon[1:])\n","print ('Correlation coefficient = {:.3f}'.format(corr_coef[0][1]))"]},{"cell_type":"markdown","metadata":{"id":"0_yx7t0XLr8h"},"source":["\n","Now calculate the 2D histogram of the adjacent pairs of pixels, and comment on the result:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6PDoJctLwYY","executionInfo":{"status":"aborted","timestamp":1708877097669,"user_tz":-60,"elapsed":7,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["xedges = np.linspace(0,250,251)\n","yedges = np.linspace(0,250,251)\n","histmat, xedges, yedges = np.histogram2d(dbaboon[1:], dbaboon[0:-1], bins=(xedges, yedges))\n","plt.imshow(histmat, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","plt.xlabel('Amplitude of current pixel')\n","plt.ylabel('Amplitude of adjacent pixel')\n","plt.title('Adjacent pixels 2D Histogram')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1PluKLBML8kW"},"source":["Comment the results (histogram of adjacent pixels and correlation coefficient):\n","\n","<font color=red>**Q1 - Answer**:  </font><br>\n","<font color=blue>\n","the histogram show us the relation between adjacent pixels. The results indicate there's a huge relation with pixels around values as 120 and 180 over 255.\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Alky2801MFKM"},"source":["#### 2.1.2. Inter-channel redundancy\n","\n","A true color image is represented using three channels.\n","Usually, when working with multispectral images, there is a strong correlation between channels. This can be clearly seen in the Baboon image and its RGB components:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJzoY-HLaWOa","executionInfo":{"status":"aborted","timestamp":1708877097669,"user_tz":-60,"elapsed":7,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["baboon_rgb  = cv2.cvtColor(baboon, cv2.COLOR_BGR2RGB)   # Convert to RGB\n","display_images(baboon_rgb,    baboon[:,:,0], 'Baboon',    'B Channel', size = 0.5)\n","display_images(baboon[:,:,1], baboon[:,:,2], 'G Channel', 'R Channel', size = 0.5)"]},{"cell_type":"markdown","metadata":{"id":"Tz_1y_y6a5AM"},"source":["A common technique for analyzing the redundancy shared between different channels is to compare each pixel in a pair of channels (R-channel and G-channel, for example). We first calculate the correlation coefficient between corresponding pixels (in the same position) in the two channels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QItTmCCUbGaj","executionInfo":{"status":"aborted","timestamp":1708877097669,"user_tz":-60,"elapsed":7,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["corr_coef = np.corrcoef(baboon_rgb[:,:,0].flatten().astype(float), baboon_rgb[:,:,1].flatten().astype(float))\n","print ('Correlation coefficient = {:.3f}'.format(corr_coef[0][1]))"]},{"cell_type":"markdown","metadata":{"id":"l7uDOjd5bgrx"},"source":["Now compute the 2D histogram of the corresponding pixels and\n","comment on the results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLQqY9prbrYs","executionInfo":{"status":"aborted","timestamp":1708877097669,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["xedges = np.linspace(0,255,256)\n","yedges = np.linspace(0,255,256)\n","fbaboon_rgb = baboon_rgb.astype(float)\n","histmat, xedges, yedges = np.histogram2d(fbaboon_rgb[:,:,0].flatten(), fbaboon_rgb[:,:,1].flatten(), bins=(xedges, yedges))\n","plt.imshow(histmat, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","plt.xlabel('R channel')\n","plt.ylabel('G channel');\n","plt.title('R-G channels 2D Histogram')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"xnOv6Xxubvv_"},"source":["<font color=red>**Q2 - Answer**:  </font><br>\n","<font color=blue>\n","on every pixel, there's a huge relation between red and green channel over values 50-200. There are a few points out of it, maybe because the differences of the channels on the mouse of the character.\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Bypk-KLkb9y2"},"source":["Consider the same image in another color space (YCbCr) and perform the same operation as before:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r97RZ7BZcJUE","executionInfo":{"status":"aborted","timestamp":1708877097669,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["baboon_ycrcb = cv2.cvtColor(baboon, cv2.COLOR_BGR2YCR_CB)\n","\n","display_images(baboon_rgb, baboon_ycrcb[:,:,0], 'Color image', 'Y Channel', size=0.5)\n","display_images(baboon_ycrcb[:,:,2], baboon_ycrcb[:,:,1], 'Cb channel', 'Cr Channel', size=0.5)"]},{"cell_type":"markdown","metadata":{"id":"AvVBNW-b5bST"},"source":["Compute the correlation coefficient:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JISQ4oc5xji","executionInfo":{"status":"aborted","timestamp":1708877097669,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["corr_coef_YCb = np.corrcoef(baboon_ycrcb[:,:,0].flatten().astype(float), baboon_ycrcb[:,:,2].flatten().astype(float))\n","corr_coef_YCr = np.corrcoef(baboon_ycrcb[:,:,0].flatten().astype(float), baboon_ycrcb[:,:,1].flatten().astype(float))\n","\n","print ('Correlation coefficient Y-Cb = {:.3f}'.format(corr_coef_YCb[0][1]))\n","print ('Correlation coefficient Y-Cr = {:.3f}'.format(corr_coef_YCr[0][1]))"]},{"cell_type":"markdown","metadata":{"id":"YjI3pM5f67e1"},"source":["Compute the 2d histograms of chanels Y and Cb and channels Y and Cr:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtCsZR0U7HmG","executionInfo":{"status":"aborted","timestamp":1708877097669,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["fbaboon_ycrcb = baboon_ycrcb.astype(float)\n","histmat1, xedges, yedges = np.histogram2d(fbaboon_ycrcb[:,:,0].flatten(), fbaboon_ycrcb[:,:,2].flatten(), bins=(xedges, yedges))\n","histmat2, xedges, yedges = np.histogram2d(fbaboon_ycrcb[:,:,0].flatten(), fbaboon_ycrcb[:,:,1].flatten(), bins=(xedges, yedges))\n","\n","fig, ax = plt.subplots(1,2)\n","plt.grid(False)\n","h = ax[0].imshow(histmat1, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","\n","hsep = 0.1\n","size = 1.0\n","dpi = h.figure.get_dpi()/size\n","h.figure.set_figwidth(histmat1.shape[1] / dpi)\n","h.figure.set_figheight(histmat1.shape[0] / dpi)\n","h.figure.canvas.resize(histmat1.shape[1] + 1, histmat1.shape[0] + 1)\n","h.axes.set_position([0, 0, 1, 1])\n","\n","ax[0].set_xlabel('Y channel')\n","ax[0].set_ylabel('Cb channel');\n","ax[0].set_title('Y-Cb channels 2D Histogram')\n","h = ax[1].imshow(histmat2, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","dpi = h.figure.get_dpi()/size\n","\n","h.figure.set_figwidth(histmat2.shape[1] / dpi)\n","h.figure.set_figheight(histmat2.shape[0] / dpi)\n","h.figure.canvas.resize(histmat2.shape[1] + 1, histmat2.shape[0] + 1)\n","h.axes.set_position([1+hsep, 0, 1, 1])\n","\n","ax[1].set_xlabel('Y channel')\n","ax[1].set_ylabel('Cr channel');\n","ax[1].set_title('Y-Cr channels 2D Histogram')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xIcrPqxg-_1E"},"source":["Discuss the differences between the correlations of the pixels in both color spaces. In which **color space** are the channels most correlated?\n","\n","<font color=red>**Q3 - Answer**:  </font><br>\n","<font color=blue>\n","in the frist picture, there's a few relation between all the Cb channel til 200 value and Y channel between 100-150.\n","\n","for other side, there's a huge relation between all the Cr channel til 200 value and Y channel between 100-150. Maybe because the beard of the charracter, appeared a point in the middle-right side of the picture.\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"iX3RwAfq_fZx"},"source":["However, if we compare the correlations between the chrominance channels (Cb and Cr) of the YCbCr image we can observe a strong correspondence between components. First calculate the correlation coefficient:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPDsRCOO_kVx","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":7,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["corr_coef_CbCr = np.corrcoef(baboon_ycrcb[:,:,2].flatten().astype(float), baboon_ycrcb[:,:,1].flatten().astype(float))\n","print ('Correlation coefficient Cb-Cb = {:.3f}'.format(corr_coef_CbCr[0][1]))"]},{"cell_type":"markdown","metadata":{"id":"KUItjJTh_tOP"},"source":["Now calculate the 2D histogram of the Cb and Cr channels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwFPgD2y_yJa","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["histmat3, xedges, yedges = np.histogram2d(fbaboon_ycrcb[:,:,2].flatten(), fbaboon_ycrcb[:,:,1].flatten(), bins=(xedges, yedges))\n","\n","plt.imshow(histmat3, interpolation='nearest', origin='lower', cmap = 'jet',\n","        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n","plt.xlabel('Cb channel')\n","plt.ylabel('Cr channel');\n","plt.title('Cr-Cb channels 2D Histogram')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"p_ppXpC4AhZk"},"source":["Despite the fact that the chrominance channels are not independent of each other, the color space transformation is equally useful. Why?\n","Before answering this question, it is better to solve subsection 2.1.4\n","\n","<font color=red>**Q4 - Answer**:  </font><br>\n","<font color=blue>\n","It is usefull to select which values of the histogram between the two channels we can use and which one we can erase. In this case, we can keep the center value and eliminate the once around\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"5_r4Hhp0Fucy"},"source":["#### 2.1.3. Temporal redundancy\n","\n","When working with videos, frames that are temporarily close share a lot of information. In particular, the values of the pixels that are in the same position in consecutive frames show a high correlation. In this section we will analyze spatial redundancy using differences between frames.\n","The Frame Difference between two images I (n) and I (n + a) is calculated as the difference between the two images:\n","```\n","Df[I(n),I(n+1)] = |I(n)-I(n+1)|\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxIX4u1uGL1s","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["im1 = cv2.imread('lab1/images/00002979.jpg')\n","im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n","\n","im2 = cv2.imread('lab1/images/00002981.jpg')\n","im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n","\n","display_images (im1, im2, 'Frame n', 'Frame n+2', size=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rR-77hEYHy59","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["frame_dif = np.abs(im1.astype(int)-im2.astype(int)).astype(np.uint8)\n","display_image(frame_dif, 'Frame Difference', size=1.0)"]},{"cell_type":"markdown","metadata":{"id":"coEPVmJlHj-i"},"source":["Discuss the meaning of the resulting Frame Difference in terms of temporal redundancy. What is the meaning of the white areas and the black areas? Include the concept of correlation in your answer.\n","\n","\n","<font color=red>**Q5 - Answer**:  </font><br>\n","<font color=blue>\n","In the picture he can see the substraction between frame 1 and frame 0. ii is something like the derivation. We can see how the players are moving between frames clearly.\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"vSyZS45mIQeM"},"source":["#### 2.1.4. Irrelevancy\n","\n","The concept of irrelevance refers to the presence of certain information in the image that is not perceptually relevant to the human visual system. Consequently, the removal of this information is generally imperceptible. To analyze this concept, we will remove one out of every two chrominance samples (one out of every two rows) from an image rendered in YCbCr space. Then we will reconstruct the image by interpolating the chrominance channels through row replication.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FBWQSfiI6X3","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.psnr_mse_maxerr import psnr_mse_maxerr\n","\n","im = cv2.imread('lab1/images/lenna.png')\n","im_rgb  = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   # Convert to RGB\n","\n","im_yvu  = cv2.cvtColor(im, cv2.COLOR_BGR2YCR_CB)\n","im_yvu2 = im_yvu.copy()\n","\n","height  = im_yvu.shape[0]\n","\n","display_images(im_rgb, im_yvu[:,:,0], 'Color image', 'Y channel', size=0.5)\n","display_images(im_yvu[:,:,2], im_yvu[:,:,1], 'U channel', 'V channel', size=0.5)\n","\n","print ('\\nDownsample chroma channels\\n')\n","display_image(im_yvu[:,:,0], 'Y channel', size=0.5)\n","display_images(im_yvu[range(0,height,2),:,2], im_yvu[range(0,height,2),:,1], 'Downsampled U channel', 'Downsampled V channel', size=0.5)\n","\n","print ('\\nReconstruct from downsampled chroma channels\\n')\n","im_yvu[range(1,height,2),:,1:3] = im_yvu[range(0,height,2),:,1:3]\n","\n","display_images(im_rgb, cv2.cvtColor(im_yvu, cv2.COLOR_YCR_CB2RGB), 'Original', 'Reconstructed', size=0.5)\n","psnr,mse,_,_ = psnr_mse_maxerr(im_rgb, cv2.cvtColor(im_yvu, cv2.COLOR_YCR_CB2RGB))\n","print ('PSNR = {}, MSE = {}'.format(psnr,mse))\n"]},{"cell_type":"markdown","metadata":{"id":"0HLNSPyiJwa5"},"source":["In the figure you can see the original and reconstructed images. What is the reduction obtained in terms of the number of samples? Do you think this reduction is proportional to the error observed between the two images? Consider the PSNR values. Is the PSNR a good measure of quality when working with images?\n","\n","<font color=red>**Q6 - Answer**:  </font><br>\n","<font color=blue>\n","The process to reduce the amount of samples for the recostruction worked with 2/3 of the original image.\n","The error observed on the image isn't proportional at all.\n","It is indeed a good quality measure\n","</font>\n"]},{"cell_type":"markdown","metadata":{"id":"WDZvjSFIJQl2"},"source":["### 2.2 Audio\n","#### 2.2.1 Temporal redundancy\n","\n","In the temporal domain, the audio signals are strongly correlated. To illustrate this concept, we will present two windows of two different signals: the first is of a tenor singing and the second is of a flute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7OjXGMDXBYc","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":6,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["!pip install soundfile   # Install necessary packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XV2rEgs0z-da","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":5,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.audio_functions import show_signals\n","import soundfile as sf\n","\n","tenor, fs = sf.read('lab1/audio/tenor.wav')\n","flute, fs = sf.read('lab1/audio/flute.wav')\n","\n","corr_coef_tenor, corr_coef_flute = show_signals(tenor, flute, 'Tenor', 'Flute', fs)\n","plt.tight_layout()\n","\n","print ('Correlation coefficient tenor = {}'.format(corr_coef_tenor))\n","print ('Correlation coefficient flute = {}'.format(corr_coef_flute))"]},{"cell_type":"markdown","metadata":{"id":"R0KBH3TNz-u1"},"source":["The two results are the autocorrelation values normalized to 1. Discuss the relationship between these values and the plots of the correlations (the center graphs).\n","\n","<font color=red>**Q7 - Answer**:  </font><br>\n","<font color=blue>\n","We can see that as much rhomboid it is the graphic, the correlation coefficient aprox. more at the value 1 normalized. That's because the fonamental frequency of the flute is much higher than the Tenor one. That one can be found between the distance of the peaks of the graphics.\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"AgKEq3-YIdNb"},"source":["The previous example only analyzed a narrow window in time (20ms) in which the signal can be considered stationary. However, the audio signals vary very quickly and the different sounds appear concatenated. To exemplify this phenomenon, we will observe a voice signal where a female voice pronounces various sounds."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAaVO_xFIsoS","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":5,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.audio_functions import show_speech\n","from IPython.display import Audio\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')\n","speech_part = speech[44000-1:100000,0]\n","\n","show_speech(speech_part,fs)\n","\n","Audio(data=speech_part, rate=fs)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kha1CfIZI2wg"},"source":["In this example we can clearly see two different classes of spectra: those with high content in low frequencies and those with higher content in high frequencies. They correspond to voiced or unvoiced sounds.\n","Can you identify them?\n","\n","<font color=red>**Q8 - Answer**:  </font><br>\n","<font color=blue>\n","Those with the long fequency band are the consonants and those with powerfull low frequencys are the vocals of the female audio\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"b6qDnalGILI4"},"source":["#### 2.2.2 Masking\n","\n","Masking occurs when the perception of a sound is affected by the presence of another louder sound that is close in frequency. This effect is due to the internal mechanism of the ear. The different sounds are translated into vibrations in specific positions of the cochlea, and if two sounds vibrate in close positions, the loudest is the one that predominates.\n","You can reproduce this effect with a synthetic sound. Listen to the following signal and comment if you can hear one or more sounds.\n","Compare your perception with the spectrogram shown in the figure.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QsCIRw1TIuD","executionInfo":{"status":"aborted","timestamp":1708877097670,"user_tz":-60,"elapsed":5,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.audio_functions import play_masked_tone\n","\n","masked_tone, fs = play_masked_tone()\n","Audio(data=masked_tone, rate=fs)"]},{"cell_type":"markdown","metadata":{"id":"zQ5TOQdvTSKD"},"source":["Compare your perception with the spectrogram shown in the figure:\n","\n","<font color=red>**Q9 - Answer**:  </font><br>\n","<font color=blue>\n","Well, like the image, i can clearly heard the frequency increase signal except the time when it have the same frequency as the costant singal sound\n","</font>\n"]},{"cell_type":"markdown","metadata":{"id":"faJIivDjTTxZ"},"source":["## 3. Transform or prediction block\n","\n","After analyzing the main characteristics of image and audio signals, we will study how we can compress these signals taking advantage of these characteristics.\n","\n","### 3.1 Transform\n","#### 3.1.1 Image\n","\n","The Transform block changes the description of the signal into a less correlated set of transformed samples that describe the same signal. It transforms the signal into a format designed to reduce its redundancy. Usually the transformation is reversible, but the operation itself does not reduce the amount of data that must be transmitted.\n","\n","Now we apply a transformation to two different blocks of the same image in gray levels (lenna_gray) and observe the images:\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"QFX1KVpBOroG","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"error","timestamp":1708877554204,"user_tz":-60,"elapsed":5578,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"bcae45a7-744a-4a8e-916e-f63c490b4c06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"]},{"output_type":"error","ename":"error","evalue":"OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-006ae78c8203>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlenna\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lab1/images/lenna.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlenna_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert to gray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdlenna\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mlenna_gray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mblock_1\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mdlenna\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m461\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["!pip install opencv-python\n","import numpy as np\n","from scipy.fftpack import dct\n","from scipy import signal\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","lenna      = cv2.imread('lab1/images/lenna.png')\n","lenna_gray = cv2.cvtColor(lenna, cv2.COLOR_BGR2GRAY) # convert to gray\n","dlenna     = lenna_gray.astype(float)\n","block_1    = dlenna[49:81,429:461]\n","block_2    = dlenna[249:281,299:331]\n","\n","B_1 = dct(dct(block_1,axis=0,norm='ortho'),axis=1,norm='ortho')\n","B_2 = dct(dct(block_2,axis=0,norm='ortho'),axis=1,norm='ortho')\n","B_1v = np.round(np.clip(B_1, 0,255)).astype(np.uint8)\n","B_2v = np.round(np.clip(B_2, 0,255)).astype(np.uint8)\n","\n","fig, ax = plt.subplots(2,2)\n","h = ax[0][0].imshow(block_1.astype(np.uint8),cmap='gray')\n","ax[0][0].set_title('Block 1')\n","ax[0][0].axis('off')\n","\n","h = ax[0][1].imshow(B_1v.astype(np.uint8), cmap='gray')\n","ax[0][1].set_title('Transformed Block 1')\n","ax[0][1].axis('off')\n","\n","h = ax[1][0].imshow(block_2.astype(np.uint8), cmap='gray')\n","ax[1][0].set_title('Block 2')\n","ax[1][0].axis('off')\n","\n","h = ax[1][1].imshow(B_2v.astype(np.uint8),cmap='gray')\n","ax[1][1].set_title('Transformed Block 2')\n","ax[1][1].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"oJPTE_ZA-mOg","executionInfo":{"status":"aborted","timestamp":1708877554205,"user_tz":-60,"elapsed":4,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wqrnyxDOsEL"},"source":["What can you say about the structure of the transformed coefficients? Is it suitable for encoding?\n","\n","<font color=red>**Q10 - Answer**:  </font><br>\n","<font color=blue>\n","i can't say to much because i have an error on the code\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"bwPD402fO31g"},"source":["Also note the autocorrelations of the original and transformed blocks, and comment on the differences in terms of spatial redundancy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7P8vfJcO8ST","executionInfo":{"status":"aborted","timestamp":1708877554556,"user_tz":-60,"elapsed":1,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["fig, ax = plt.subplots(2,2)\n","ax[0][0].contourf(range(-31,32), range(-31,32), signal.correlate2d(block_1,block_1),100, cmap='jet')\n","ax[0][0].set_title('Autocorrelation Block 1')\n","ax[0][0].axis('equal')\n","\n","ax[0][1].contourf(range(-31,32), range(-31,32), signal.correlate2d(B_1,B_1),100, cmap='jet')\n","ax[0][1].set_title('Autocorrelation Transformed Block 1')\n","ax[0][1].axis('equal')\n","ax[1][0].contourf(range(-31,32), range(-31,32), signal.correlate2d(block_2, block_2),100, cmap='jet')\n","ax[1][0].set_title('Autocorrelation Block 2')\n","ax[1][0].axis('equal')\n","ax[1][1].contourf(range(-31,32), range(-31,32), signal.correlate2d(B_2, B_2),100, cmap='jet')\n","ax[1][1].set_title('Autocorrelation Transformed Block 2')\n","ax[1][1].axis('equal')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gvyKQNKEO8gK"},"source":["<font color=red>**Q11 - Answer**:  </font><br>\n","<font color=blue>\n","i can't answer that\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"4KXLW05fPGKa"},"source":["#### 3.1.2 Audio\n","In order to exploit the benefits of the transformation, the audio signals must be divided into short windows, where they can be considered stationary. We will analyze a particular transformation (the discrete cosine transformed DCT) into a speech signal with 20ms windows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhygnbY9PUaM","executionInfo":{"status":"aborted","timestamp":1708877554557,"user_tz":-60,"elapsed":2,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.audio_functions import transform_speech\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')\n","speech     = speech[44000-1:100000,1]\n","\n","transform_speech(speech,fs)"]},{"cell_type":"markdown","metadata":{"id":"_aFelCTzPdOa"},"source":["The figure shows a transformed signal and two portions of the signal, with the corresponding magnitudes of the transformed coefficients.\n","Can you tell what kinds of portions of the signal have been transformed, voiced or unvoiced? What can you say about the structure of the significant coefficients? Is it suitable for encoding?\n","\n","<font color=red>**Q12 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"kbzQU-vLmBf5"},"source":["### 3.2 Prediction\n","#### 3.2.1 Image\n","\n","Instead of transforming a signal, another option is to use a predictive model in which we take advantage of spatial redundancy. For example, we can predict each pixel in the image by averaging some of its neighboring pixels (in the example, three pixels from the previous row and the previous pixel from the current row).\n","\n","![image2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHMAAABzCAIAAAAkIaqxAAAAAXNSR0IArs4c6QAAAAlwSFlzAAAOwwAADsMBx2+oZAAAAnVJREFUeF7t2+tugkAUReHSJ/fNLYaEmqJymbOcTV39rXvox84UJj3D9Xr98gcQ+AYyjbwJKEv1YJh3g2EYqEU+KffX8172crm8B2Fc6L+uNXu6G1BdUlZZSoDKtbPKUgJUrp1VlhKgcu2sspQAlWtnlaUEqFw7qywlQOXaWWUpASrXzipLCVC5dlZZSoDKtbPKUgJUrp1VlhKgcu2sspQAlWtnlaUEqFw7qywlQOXaWWUpASrXzipLCVC5dpaSdU6hWLb/nMLb5tDGyZYua7kbFHd2jguQJaaliMydt6C37ERQC0Fk7mQdP95bdr7iKtyqnP2Uf77RW/Z+nrod5T6h96R2b9nxRlfhJrHG7AbtuGGsMbKNzc1jTZI9jBvJGiZ7ADeVNU92F24wa6TsRtxs1lTZVdx41mDZF7hnYM2WfYh7EtZ42RfN7f3yunquEPB2u3qNS8R41jN0dtU99QNn6OzyDKz9VIy/H/Gyz/5kxeNmyy5Z20/F+LZOKwTLPmvrSXBTZV8/t54BN1J2y+tAPG6e7BbWaSfLxg2T3c4aj5sku5c1GzdG9hhrMG6GbAtrKm6AbDtrJG5v2SrWPNzesvOTU9XBYHng0bfh3rLTY2kV69zc2sBDuAGyh647/0vOKRTfo8dzCl3+n7/4N1vEOadAC787332WEldWWUqAyrWzylICVK6dVZYSoHLtrLKUAJVrZ5WlBKhcO6ssJUDl2lllKQEq184qSwlQuXZWWUqAyrWzylICVK6dVZYSoHLtrLKUAJVrZ5WlBKhcO6ssJUDl2lllKQEq184qSwlQuc4pFMs+mFMoXuHj49xnqQooS8n+AGv/vcPzsm1EAAAAAElFTkSuQmCC)"]},{"cell_type":"markdown","metadata":{"id":"gDd3EnDVmfMw"},"source":["Why don't we use the blank pixels in the figure to predict pixel X?\n","\n","<font color=red>**Q13 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"p2RII67lmlR9"},"source":["Thus, it is only necessary to send the prediction error, that is, the difference between the original value and the prediction value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MuIhFZQmudl","executionInfo":{"status":"aborted","timestamp":1708877554557,"user_tz":-60,"elapsed":2,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.predictive_models import predictive_model_image\n","\n","lenna      = cv2.imread('lab1/images/lenna.png')\n","lenna_gray = cv2.cvtColor(lenna, cv2.COLOR_BGR2GRAY).astype(float) # convert to gray\n","\n","predictive_model_image(lenna_gray)"]},{"cell_type":"markdown","metadata":{"id":"BG6iPR-89_ZS"},"source":["What kind of redundancy is exploited with the previous predictive model example? In which parts of the image are the most significant errors concentrated? Why?\n","\n","<font color=red>**Q14 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"qMgLO6lw-DCl"},"source":["Let's consider an encoding scheme. If a predictive model is used, what information must the transmitter send to the receiver? What is the procedure that the receiver must follow to recover the original image?\n","\n","<font color=red>**Q15 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"-LZev8Kt-Pgy"},"source":["#### 3.2.2 Audio\n","\n","A simple model for predicting audio signals is to consider only the previous sample, and transmit only the prediction error\n","```\n","e (n) = s(n) - s(n-1))\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oytYWtrQ-gSH","executionInfo":{"status":"aborted","timestamp":1708877554557,"user_tz":-60,"elapsed":2,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.predictive_models import predictive_model_speech\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')\n","speech     = speech[44000-1:100000,1]\n","predictive_model_speech (speech, fs)"]},{"cell_type":"markdown","metadata":{"id":"epNAckk5-jdk"},"source":["In this scheme, there are times when the errors are large, and others, regardless of the amplitude of the signal, when they are negligible.\n","From looking at the figure, can you identify the reason? Is there a relationship with the voiced / unvoiced characteristics of phonemes?\n","\n","<font color=red>**Q16 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"eDwz-bmDEJkK"},"source":["## 4. Quantization block\n","### 4.1 Image\n","\n","This block reduces the precision of the transform block output according to a pre-set fidelity criterion. The goal is to remove irrelevant information from the compressed representation.\n","\n","The quantifier represents a range of values ​​from a sample transformed by a single value in the range. The set of output values ​​are the quantization indices. This step is responsible for the losses in the system and therefore determines the degree of compression achieved (here we are assuming scalar quantization, but it is also possible to group a set of transformed samples into a vector and perform vector quantization).\n","\n","This operation is irreversible, and therefore should be omitted when lossless compression is desired.\n","\n","Next, we will perform a quantization on a block of the image using different quantization steps.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"LO3EMcCnt-rA","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1708877657885,"user_tz":-60,"elapsed":351,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"d88ee907-5c34-4c2e-865d-95b1c2b26c26"},"outputs":[{"output_type":"error","ename":"error","evalue":"OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-dbcc8d5b5223>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlenna\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lab1/images/lenna.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlenna_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert to gray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdlenna\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mlenna_gray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["import cv2\n","from scipy.fftpack import dct\n","from skimage.measure import shannon_entropy\n","import numpy as np\n","\n","lenna      = cv2.imread('lab1/images/lenna.png')\n","lenna_gray = cv2.cvtColor(lenna, cv2.COLOR_BGR2GRAY) # convert to gray\n","dlenna     = lenna_gray.astype(float)\n","\n","block = dlenna[129:161,99:131]\n","B = dct(dct(block,axis=0,norm='ortho'),axis=1,norm='ortho')\n","\n","kk   = 0\n","err  = np.zeros(1901)\n","ent  = np.zeros(1901)\n","\n","for ii in np.linspace(1,20,1901):\n","    Bq = np.round(B/ii)*ii\n","    err[kk] = np.sum(np.abs(B-Bq)**2)\n","    ent[kk] = shannon_entropy(Bq)\n","    kk = kk + 1\n","\n","plt.plot(ent,10*np.log10(err))\n","plt.xlabel('Entropy')\n","plt.ylabel('MSE [dB]')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"JIo0EmumqWV4"},"source":["The figure presents the relationship between error and entropy.\n","\n","The purpose of the figure is to show the impact of the quantizer on the relationship between compression rate and distortion (rate and distortion). However, since the compression calculation involves one encoding step, the compression rate will be approximated by entropy."]},{"cell_type":"markdown","metadata":{"id":"2QRUoTXvq8wA"},"source":["Discuss the relationship between the entropy of the signal and the compression rate that can be obtained by encoding it.\n","\n","<font color=red>**Q17 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"b8d05jlVt4_u"},"source":["Discuss the relationship between quantization levels and the entropy of the output signal relative to the entropy of the input signal in the quantization step.\n","\n","<font color=red>**Q18 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Psz34iCHJ7TB"},"source":["Discuss the relationship between compression rate and distortion (error) depending on the number of quantization levels. Hint: Remember that, quantitatively, the compression rate can be approximated by the entropy of the signal.\n","\n","<font color=red>**Q19 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Y7nAAfS1KG4t"},"source":["Why do you think it is important to get a small entropy value? Is it possible to achieve an arbitrarily small compression rate? Why?\n","\n","<font color=red>**Q20 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"flMuSxflKOSC"},"source":["### 4.2 Audio\n","\n","To demonstrate the effects of audio quantization, we will examine two different types of quantization. The first is the quantization of the signal itself with two different types of quantizer: one uniform and the other non-uniform. The L parameter is the quantization step and should be adjusted to vary the quality (start with L = 0.01 and change the value)."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"VvRQO5cuKa-y","colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"status":"error","timestamp":1708877657886,"user_tz":-60,"elapsed":5,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}},"outputId":"021dd210-80ff-44f9-f4c2-f8f02491d7e8"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'lab1'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b8b19ef94a7a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlab1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulaw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoundfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspeech\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lab1/audio/speech.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspeech\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mspeech\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m44000\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab1'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from lab1.code.audio_functions import mulaw\n","import soundfile as sf\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')\n","speech     = speech[44000-1:100000,1]\n","\n","L = 0.01\n","sq, imu = mulaw(speech, fs, L)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIBKddVtt4I5","executionInfo":{"status":"aborted","timestamp":1708877657886,"user_tz":-60,"elapsed":4,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["Audio(data=sq,  rate=fs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKgdsa_6t6IH","executionInfo":{"status":"aborted","timestamp":1708877657886,"user_tz":-60,"elapsed":4,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["Audio(data=imu, rate=fs)"]},{"cell_type":"markdown","metadata":{"id":"6VPJRBOTKemF"},"source":["Which one sounds better? Note that the **imu** signal has been logarithm-transformed before quantization. Can this transformation affect perception? (Hint: think about what happens to small samples and ear sensitivity)\n","\n","\n","<font color=red>**Q21 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"0gSf_8njK0v_"},"source":["The second type of quantization will be the quantization of the prediction error. Only a uniform quantizer will be applied here. Adjust the L parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKVTVSLYLAGo","executionInfo":{"status":"aborted","timestamp":1708877657886,"user_tz":-60,"elapsed":4,"user":{"displayName":"Josep Esquerrà","userId":"17351964510526470848"}}},"outputs":[],"source":["from lab1.code.audio_functions import quantize_error\n","\n","speech, fs = sf.read('lab1/audio/speech.wav')\n","speech     = speech[44000-1:100000,0]\n","\n","L=0.01\n","rs, fs = quantize_error(speech, fs, L)\n","Audio(data=rs,  rate=fs)"]},{"cell_type":"markdown","metadata":{"id":"o36GVGcSLFLl"},"source":["Is the quality of this signal comparable to either of the two above?\n","\n","<font color=red>**Q22 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"2TdIRvj1LIUs"},"source":["## 5. Entropy coding\n","\n","This block produces the final bistream. The entropic encoder generates a fixed or variable length code to represent the output of the quantizer, and encodes the output according to the code. In many cases a variable length code is used: the shortest code words are assigned to the most frequent output values, minimizing code redundancy. This operation is reversible.\n","\n","Next, we have generated two signals that come from two different sources. One of the sources has a Gaussian distribution, while the other has a uniform distribution. Both sources can generate nine different symbols. Then the symbols have been encoded using a Huffman encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S998LMdXcR2v"},"outputs":[],"source":["!pip install huffman # Install necesssary package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DD6sJ9WLc30"},"outputs":[],"source":["import pickle\n","import huffman\n","\n","with open('lab1/random_signals.pkl', 'rb') as fd:\n","  rd = pickle.load(fd)\n","n1 = rd['n1'].squeeze()\n","n2 = rd['n2'].squeeze()\n","\n","symbols = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4]) # Distinct data symbols appearing\n","x1,_ = np.histogram(n1,9)\n","x2,_ = np.histogram(n2,9)\n","N = n1.shape[0]\n","p1 = x1/N\n","p2 = x2/N\n","dict1 = huffman.codebook(zip(symbols,p1))\n","dict2 = huffman.codebook(zip(symbols,p2))\n","\n","hcode1 = ''.join([dict1[ii] for ii in n1]) # Encode the data.\n","hcode2 = ''.join([dict2[ii] for ii in n2]) # Encode the data.\n","\n","length1 = len(hcode1)\n","length2 = len(hcode2)\n","\n","entropy1 =  -np.sum(p1 * np.log2(p1))\n","average_length_1 = len(hcode1)/N\n","\n","entropy2 = -np.sum(p2 * np.log2(p2))\n","average_length_2 = len(hcode2)/N\n","\n","print ('Entropy 1 = {:.2f}, Average length 1 = {:.2f}'.format(entropy1, average_length_1))\n","print ('Entropy 2 = {:.2f}, Average length 2 = {:.2f}'.format(entropy2, average_length_2))\n"]},{"cell_type":"markdown","metadata":{"id":"ZErL51gLLh1f"},"source":["From the analysis of the entropy values, identify which signal corresponds to the Gaussian distribution and which to the uniform distribution. Also compare the entropy values with the mean length. Do these values satisfy the condition\n","```\n","H(x) <= L <= H(x)+1?\n","```\n","<font color=red>**Q23 - Answer**:  </font><br>\n","<font color=blue>\n","(answer here)\n","</font>"]}],"metadata":{"colab":{"collapsed_sections":["kp5iKtXTJRlt","vSyZS45mIQeM","WDZvjSFIJQl2","2TdIRvj1LIUs"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}